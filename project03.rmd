---
title: 'Project #3 Deliverable'
author: "Steffani Gomez"
date: "August 6, 2017"
output: 
  html_document:
    css: project03style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<div id="logos">
  <img src="http://www.brewerydb.com/img/logo_small.png" id="brewerylogo">
  <img src="https://www.ratebeer.com/images/logos/login-logo.png" id="ratebeerlogo">
</div>

# Analysis of Beers and Breweries Around the World

## Why You Should Care

I wanted to do a variety of things for the final project and push myself to use the most I can out of R and use my computer science skills.

North Carolina is arguably the state with the most breweries in the country. Beer is one of the three kinds of liquor you can buy (wine, liquor, beer) and the advertising for it is everywhere. Down a highway or on television, there is almost certainly some kind of advertising for some sort of beer, perhaps one you've never heard of. And if you've ever seen the SuperBowl, you've definitely seen the [halftime commercials for beer](https://www.youtube.com/watch?v=aYeXldMEpV8). If that isn't enough to convince anyone, Anheuser-Busch InBev SA/NV is the largest brewery in the world and is one of largest and most profitable fast-moving consumer goods companies in the world, with \$45.5 billion in revenue last year and a projected revenue of $55 billion this year. Craft beer is growing in popularity, so it's safe to say quite a few people care about beer and the money it brings in. 

## Project Summary

For this final project I want to focus on beer and breweries in the United States. There will be three large aspects to this project: data wrangling, data visualization, and prediction, the Big Three of data science, with prediction being the side of machine learning I want to focus on, rather than classification. I want to explore the spread of breweries in the United States and I'd like to see if there is a viable prediction algorithm for beers, given a beer/brewery and its characteristics, can we predict how well-liked the beer will be? This has huge implications for owners of breweries, if ratings can be predicted, then one can aim to create a beer that will be well liked, and if certain locations are more open to certain kinds of beer, then marketing for that specific kind of beer should be increased in those areas. This is just an example of what the possible results of this project can mean in the real world. 

## Datasets
I will be using data from two websites, [BreweryDB](http://www.brewerydb.com/) and [RateBeer](https://www.ratebeer.com/). BreweryDB will be my main source of data, with RateBeer providing ratings of the beers based on community reviews. These two websites are well known and are often used in "beer analytics" and have reputable communities behind them, with revisions being made every day. BreweryDB maintains a staff that checks the authenticity of beers and breweries and the statistics of them, and RateBeer will provide the ratings and the number of reviewers so we can determine whether to trust an average rating when there has only been 2 reviewers for example. Getting data out of these websites is where the data wrangling portion of this project is. 

BreweryDB provides just about all the information you could want about a brewery and the different beers it produces. The information I will be using from BreweryDB is: 

  - beer name
  - beer category (categories based off of the [Brewers Association Style Guidelines](https://www.brewersassociation.org/resources/brewers-association-beer-style-guidelines/))
  - beer style (the styles of beers within each category)
  - brewery location
  - brewery name
  - brewery type
  - social media accounts of the breweries
  
RateBeer will only be used to associate ratings, which it provides, to the beers found in BreweryDB. 

I will be using the [BreweryDB API](http://www.brewerydb.com/developers/docs) to access its database and I will be accessing it from my R program. RateBeer is much trickier since its API has been down for a number of years now. However, thanks to GitHub and open-sourcing, there is a kind soul who created a [Python wrapper](https://github.com/alilja/ratebeer) to scrape data off of RateBeer and return it in the same manner that an API would. So I will write a script in Python to get the ratings data and call that script from R. I foresee that the main issue will be differences in beer names between the two website and I'm sure that some beers will exist in one dataset but won't exist in the other, but that's what data wrangling is all about!

## Methodology
There are three main results I hope to deliver by the end of this project:

  1. Create an interactive geovisualization of the location of different breweries in the United States and the beers that they offer, with filters for regions, beer category, beer style, age of the breweries, and kinds of breweries. 
  2. Create an interactive geovisualization for the location of different breweries and the ratings of the beers and be able to filter by location and ratings. The goal of these visualizations is to explore the data and see if there are any patterns that we can use to our advantage. 
  3. Based on what information the visualizations offer, create a prediction algorithm for beer ratings. Given location, beer category, beer style, alcohol content (ABV, IBU), presence of a social media account, and brewery type, can I create an accurate recommendation algorithm? For this, I will use regression. 
  
## Schedule
Because this project is very involved, I am planning on starting the project this week. My plan of action is as follows:

  - Week 1 7/26 - 8/02: Get data out of the BreweryDB API and the RateBeer Python wrapper
  - Week 2 8/02 - 8/09: Fix discrepancies between the two datasets and begin to work on the visualizations
  - Week 3 8/09-8/16: Finish up the visualizations and begin working on the regression formula
  - Week 4 8/16-8/23: Finish the regression algorithm and add finishing touches to the project
  
## DATA WRANGLING 
To access BreweryDB data on breweries and their beers, as well as their locations, I used the BreweryDB API. This API used an API key to access the information in the API, and I built 4 main data dictionaries about 66K+ beers and the breweries that make them. 

In the BreweryDB framework, beers have associated brewery, style, and category data. The BreweryDB website is continually updated with a staff constantly checking authenticity of beers and their breweries. However, some breweries are more visible than others, especially those with websites versus those based in foreign countries, and accordingly there can be missing information about some of the beers and breweries, and that will be discussed in detail later on. The beer styles and categories based off of the [Brewers Association Style Guidelines](https://www.brewersassociation.org/resources/brewers-association-beer-style-guidelines/).

A beer can have several breweries that make it, and a brewery can be tied to several locations, such as a main brewery and its microbreweries (check if this is a thing). However, the relationship of location to physical coordinates is a one-to-one, with every unique location given a unique id, denoted *locationId*. The beer to breweries relationship is one-to-many and the breweries to locations relationship is a one-to-many relationship as well. This is shown in the figure below:

[insert figure showing the one-to-many relationships by foreign keys, find some schema software uml diagrams?]
  
BreweryDB has over 66K+ unique beers, with other 1300+ pages of JSON data about beers and their breweries. On my laptop it took over an hour to pull all the data about the beers. I have written code that updates all the dictionaries every time that the script is run, rewriting the dictionaries kept on disk if need be, All four separate data dictionaries are stored locally, simply because the time required to build it from scratch is too much to wait for every time I want to access the data. The merged copies are built from scratch because it can be done nearly instantaneously. The beer data dictionary has two __foreign__ keys, ***breweryId*** from the breweries data dictionary and ***styleId*** from the beer styles dictionary. The beer data is merged with both the beer styles dictionary and the breweries data dictionary.

Getting beer data out of the BreweryDB API was much more complicated than previously anticipated. I had planned to use the *tidyjson* package, but found that there was a bug that had arisen recently that no one had a quick fix for, specifically when attempting to access nested JSON lists and a strange issue with the *dplyr* package. (add more info here if anything). The beer data straight out of the API is ordered in the following manner:

```{r, eval=FALSE}
'{
  "status" : "success",
  "numberOfPages" : 225,
  "data" : [
    {
      "servingTemperatureDisplay" : "",
      "labels" : {
        "medium" : "http://s3.amazonaws.com/",
        "large" : "http://s3.amazonaws.com/",
        "icon" : "http://s3.amazonaws.com/"
      },
      "style" : {
        "id" : 15,
        "category" : {
          "updateDate" : "",
          "id" : 5,
          "description" : "",
          "createDate" : "2012-01-02 11:50:42",
          "name" : "Bock"
        },
        "description" : "",
        "ibuMax" : "27",
        "srmMin" : "14",
        "srmMax" : "22",
        "ibuMin" : "20",
        "ogMax" : "1.072",
        "fgMin" : "1.013",
        "fgMax" : "1.019",
        "createDate" : "2012-01-02 11:50:42",
        "updateDate" : "",
        "abvMax" : "7.2",
        "ogMin" : "1.064",
        "abvMin" : "6.3",
        "name" : "Traditional Bock",
        "categoryId" : 5
      },
      "status" : "verified",
      "srmId" : "",
      "beerVariationId" : "",
      "statusDisplay" : "Verified",
      "foodPairings" : "",
      "breweries":  [{
        "id" : "KlSsWY",
        "description" : "",
        "name" : "Hofbrouwerijke",
        "createDate" : "2012-01-02 11:50:52",
        "mailingListUrl" : "",
        "updateDate" : "",
        "images" : {
          "medium" : "",
          "large" : "",
          "icon" : ""
        },
        "established" : "",
        "isOrganic" : "N",
        "website" : "http://www.thofbrouwerijke.be/",
        "status" : "verified",
        "statusDisplay" : "Verified"
      }],
      "srm" : [],
      "updateDate" : "",
      "servingTemperature" : "",
      "availableId" : 1,
      "beerVariation" : [],
      "abv" : "6",
      "year" : "",
      "name" : "\"My\" Bock",
      "id" : "HXKxpc",
      "originalGravity" : "",
      "styleId" : 15,
      "ibu" : "",
      "glasswareId" : 5,
      "isOrganic" : "N",
      "createDate" : "2012-01-02 11:51:13",
      "available" : {
        "description" : "Available year round as a staple beer.",
        "name" : "Year Round"
      },
      "glass" : {
        "updateDate" : "",
        "id" : 5,
        "description" : "",
        "createDate" : "2012-01-02 11:50:42",
        "name" : "Pint"
      },
      "description" : "Amber, malty and not too heavy, all around favorite even for the drinkers of the yellow fizzy stuff"
    },
    ...
  ],
  "currentPage" : 1
}'
```
The data frame created directly from the JSON data has ***breweries*** defined as a list of lists, key-value pairs, encoded as a string, for each beer item. The key to making the data frame tidy was to extract information from the breweries and add it as proper columns/variables in the beers data drame, and removing extraneous information; as seen above, the breweryDB API returns a lot of data, a lot of which we aren't interested in. While trying to do this, I quickly ran into issues stemming from the *dplyr* and the *tidyjson* packages documented [here](https://github.com/tidyverse/dplyr/blob/master/revdep/problems.md) and [here](https://github.com/MarkEdmondson1234/googleAnalyticsR/issues/88), receiving this error message:
```{r, echo=FALSE}
suppressPackageStartupMessages(library(jsonlite)) # for working with JSON data
suppressPackageStartupMessages(library(tidyverse)) # to transform and clean data

source("BreweryDBRWrapper.R")

# my BreweryDB API key
breweryDBKey <- "11b192faea1a549172fe2423db077bc5"

beersRequestData <- BreweryDB_endpoint(breweryDBKey, "beers", options = list(p = as.character(1))) %>%
  content(as = "text", encoding = "UTF-8") %>% fromJSON(simplifyDataFrame = TRUE)
```


```{r, echo=TRUE, eval=FALSE}
library(tidyjson)
library(tidyverse)

beers <- beersRequestData$data

beers %>% 
  gather_array %>% 
  spread_values(name = jstring("name"))

Error in eval(assertion, env) : 
  argument "json.column" is missing, with no default
```

Downgrading the *dplyr* package to version 0.5.0 and even downgrading the *tidyjson* package to version 0.2.1 did not resolve the issue, so I had to devise my own way of accessing the information and making the data frame tidy, using R's apply functions, also known as group of mapping functions, explained beautifully in this [Stack Overflow post](https://stackoverflow.com/questions/3505701/r-grouping-functions-sapply-vs-lapply-vs-apply-vs-tapply-vs-by-vs-aggrega). To extract any data located in a list in a column, I used the following code:

```{r, eval=FALSE}
beers$breweryId <- lapply(beers$breweries, FUN = function(x) { paste(x$id, collapse = " ") })
```

turning a list of brewery ids located in the list of breweries into a string of brewery ids separated by a space, for easy separation of a beer id, 1 observation, into several observations of that beer into a beer and its breweries in the main data dictionary later. 

The final beer data dictionary has the following variables:

```{r, echo=FALSE}
variables <- c("beerId", "beerName", "beerDescription", "abv", "ibu", "styleId", "categoryId", "breweryId")
descriptions <- c("the id of the beer", "the name of the beer", "the official description of the beer", "the alcohol by volume of the beer (expressed as a percentage", "the IBU (international bittering unit) value of the beer, a measure of how bitter a beer is", "the style id of the beer", "the category id of the style id", "the id of the brewery that makes the beer")
beerstable <- data.frame(variables, descriptions)
library(knitr)
kable(beerstable)
```

with a beer id and a brewery id acting as __primary__ keys of the beers data frame, meaning that the two together uniquely identify one observation in the data frame. 


The brewery data dictionary was assembled in a similar manner to the beers data dictionary, with *locations* being the list nested in the list of data items in the JSON, and *locationId* being the list of ids associated with each brewery id. The final brewery data dictionary has the following variables, with a brewery id and a location id as __primary__ keys of the data frame:

```{r, echo=FALSE}
variables <- c("breweryId", "breweryName", "breweryDescription", "locationId")
descriptions <- c("the id of the brewery", "the name of the brewery", "the description of the brewery", "the location id associated with a brewery id (a brewery can have several locations")
breweriestable <- data.frame(variables, descriptions)
library(knitr)
kable(breweriestable)
```
  
Locations are in a separate data dictionary of their own, partially because the BreweryDB API had the locations as their own dictionaries and because there's so much information associated with a location id. The variables in the final locations data dictionary are as follows, with *locationId* being the __primary__ key of the data frame:

```{r, echo=FALSE}
variables <- c("locationId", "locationName", "streetAddress", "locality", "region", "postalCode", "latitude", "longitude", "locationTypeDisplay", "isPrimary", "countryIsoCode", "breweryId")
descriptions <- c("the id of a particular location (geophysical location)", "the name of a location, usually street name", "the address and number of a location", "the city of the location", "the ztate of the region", "the postal code of the location", "the latitude coordinates of the location", "the longitude coordinates of the location", "the kind of location it is: restuarant vs microbrewery for example", "whether that particular location is the primary location for a particular brewery", "the two character country code of a location", "the brewery id of the brewery associated with this particular location")
locationstable <- data.frame(variables, descriptions)
library(knitr)
kable(locationstable)
```
  
  Finally, I created a styles to categories data dictionary of all the different styles and categories and their mappings, associating styles and style information like the range of alcohol per beer volume content fo that particular style, with *styleId* being the __primary__ key for the data frame. The variables in this dictionary are:

```{r, echo=FALSE}
variables <- c("styleId", "categoryId", "name", "shortName", "description", "ibuMin", "ibuMax", "abvMin", "abvMax", "srmMin", "srmMax", "ogMin", "ogMax", "fgMin", "fgMax", "categoryName")
descriptions <- c("the style id", "the id of the category that style belonged to", "the name of the style", "the name of the style, shortened", "the description of that style", "the minimum international bitterness value of the style", "the maximum international bitterness value of the style", "the minimum alcohol per beer volume content of the style", "the maximum alcohol per beer volume content of the style", "the minimum in the typical SRM range for this style", "the maximum in the typical SRM range for this style", "the minimum in the typical original gravity range for this style", "the maximum in the typical original gravity range for this style", "the minimum in the typical final gravity range for this style", "the maximum in the typical final gravity range for this style", "the name of the category the style belongs to")
stylestable <- data.frame(variables, descriptions)
library(knitr)
kable(stylestable)
```

Unfortunately, the Python wrapper designed to get RateBeer information by scraping the website did not work as planned. It seems that in the recent weeks, RateBeer has changed the encoding of their website, rendering the wrapper unusable as the encoding of the script and the website are uncompatible, and searching by beer or sending any kind of string as a GET request is unrecognizable by the website and the data returned is unreliable. So this project will focus on exploring the BreweryDB data and building some cool interactive visualizations from it. [CHECK THIS OUT]

Most of us know what abv and ibu is, since these statistics are often displayed on the beer iteself. But what is SRM, and original and final gravity and what does that have to do with beer? 

# SRM & Original and Final Gravity

SRM is short for the [Standard Reference Method](http://brewwiki.com/index.php/Standard_Reference_Method), the color system used by breweries for finished beer and malts. 

```{r kable, echo=FALSE}
ranges <- c("1.0 - 2.0", "2.0 - 3.0", "3.0 - 4.0", "4.0 - 6.0", "6.0 - 8.0", "8.0 - 10.0", "10.0 - 13.0", "13.0 - 17.0", "17.0 - 20.0", "20.0 - 24.0", "24.0 - 29.0", "29.0 -35.0", "35.0 - 40.0", "40.0+")
examples <- c("Pale lager", "Pilsener", "Blonde Ale", "Weissbeer", "India Pale Ale", "Saison", "English bitter", "Double IPA", "Amber Ale", "Brown Ale", "Porter", "Stout", "Foreign Stout", "Imperial Stout")
srmtable <- data.frame(ranges, examples)
library(knitr)
kable(srmtable, digits=3)
```

Gravity, in the context of brewing alcohol, is the density of the [wort](https://en.wikipedia.org/wiki/Wort) or [must](https://en.wikipedia.org/wiki/Must) compared to water. Original gravity refers to the gravity of the liquid before fermentation, and final gravity is its gravity after fermentation. 

# How Beer is Made
<div id="howbeer">
  <img src="http://www.eightdegrees.ie/wp-content/uploads/2011/05/the_brewery_process.jpg" id="howbeerismade"><br>
  <a href="http://www.eightdegrees.ie/wp-content/uploads/2011/05/the_brewery_process.jpg">Image Source</a>
</div>

Beer is mainly made out of four ingredients: water, yeast, a grain, such as barley, and hops. 

  1. **Malting**: The grain is prepared for boiling by being steeped in water and allowed to partially germinate, softening the kernel. This isolates the natural enzymes that will later on break down the starch into sugar. This process is stopped by heating, drying out and cracking the barley, turning it into malted barley.
  2. **Mashing**: The malted barley is soaked in hot water, allowing the natural enzymes in the grain to turn starch into sugar for the yeast to consume later during fermentation.
  3. **Sparging**: After the starch has been turned to sugar, the liquid becomes full of sugar and is separated from the grains in a process called lautering and the liquid that is leftover is now called *wort*.
  4. **Boiling and Cooling**: The wort is placed in a boil kettle and is boiled to kill any micro-organisms left in the liquid and is also when hops and spices are added over the period of about an hour. Hops provide bitterness in beer, balancing out the sugar. After, the wort is then quickly cooled down, strained, and filtered so that yeast can be added to it without killing it from the heat of the liquid. 
  5. **Fermentation**: The waiting period for the yeast to consume the sugar and turn it into alcohol, which is typically a few weeks. 
  6. **Carbonation or Aging**: After the fermentation period, the beer is still uncarbonated. It can either be artifically carbonated or it can be "bottle conditioned" and allowed to age with the CO2 produced by the yeast. homebrewers..
  
## DATA CLEANING
  
  WHAT ABOUT SRM??
 
Now that we have all of our data, we might want to take a look at the distribution of the most distinguishable beer characteristics, abv (alcohol per beer volume, expressed as a percentagage out of 100) and ibu (international bitterness unit value, which is a measure of how bitter the beer is).

```{r, echo=FALSE}
beersFile <- "data/beers.rds"
beers <- read_rds(beersFile)
summary(beers$abv)
```

THe summary statistics of the abv of al the beers reveal that the maximum abv is 308. Since a percent
is out of 100, everything above 100 doesn't make sense and we can remove all the beers whose abv is above 100 since the credibility of that beer is now questionable. Thankfully, there is only 1 beer whose abv is above 100, and we dispose of that observation. Now we visualize the distribution to get a better idea of the characteristics of the abv.

```{r, echo=FALSE, warning=FALSE}
beers <- beers %>% filter(id != "EHPIi4")
beers %>% ggplot(aes(abv)) + geom_histogram(binwidth = 1, colour = "BLACK")
```

It seems that the majority of beer abv is between 0 and 20, so let's visualize observations within that range.

```{r, echo=FALSE}
beers %>% filter(abv <= 20) %>% ggplot(aes(abv)) + geom_histogram(colour = "BLACK", binwidth = 1)
```


<!--# if we wanted to see the distribution of location types among the dataset
beersBreweriesLocations %>% ggplot(aes(x = locationTypeDisplay)) + geom_bar()
#beersBreweriesLocations %>% ggplot(aes(abv)) + geom_histogram(binwidth = 1) + facet_wrap(~ countryIsoCode)

# the distribution could be considered normal, with a skewness of 3.428445

#skewness(beers$abv, na.rm = TRUE)-->

Let's take a look at the IBU distributions of all the beers.
```{r, echo=FALSE}
summary(beers$ibu)
```
As seen in this [chart](https://www.brewersfriend.com/2009/01/24/beer-styles-ibu-chart-graph-bitterness-range/), ibu doesn't normally go above 120, with the units being parts per million, and this [article](https://beerconnoisseur.com/articles/whats-meaning-ibu) claiming that the human tongue can't distinguish past 110 IBUs. 

```{r, echo=FALSE}
beersHighIBUs <- beers %>% filter(ibu > 120) %>% nrow
```

There are `r beersHighIBUs` beers above 120 IBUs, and googling of the first few beers reveals that these are authentic beers, so no observations will be removed for wrong IBU range, but these observations will be left out of exploratory data analysis visualizations to avoid skewing the scale of data. 

```{r, echo=FALSE}
beers %>% filter(ibu <= 120) %>% select(ibu) %>% summary()

beers %>% filter(ibu <= 120) %>%
  ggplot(aes(ibu)) + geom_histogram(colour = "BLACK", binwidth = 5)

beers %>% filter(!is.na(srmId) & ibu <= 120) %>% 
  ggplot(aes(srmId)) + geom_histogram(colour = "BLACK", binwidth = 2)

beers %>% filter(abv <= 20 & ibu <= 120) %>% 
  ggplot(aes(abv)) + geom_histogram(colour = "BLACK", binwidth = 1)
```

It seems that beers are either usually fairly light colored or very dark colored, so either a pale lager or a strong stout.
